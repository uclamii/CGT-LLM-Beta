# RAG Chatbot with Vector Database

A production-ready Retrieval-Augmented Generation (RAG) chatbot built in Python that runs efficiently on Apple Silicon (M2 Max) using MPS acceleration. The system processes documents from local folders, answers questions using a vector database for fast retrieval, and outputs results to CSV files.

## üöÄ Features

- **‚ö° Lightning Fast Retrieval**: ChromaDB vector database with sub-second query times
- **üß† Multi-Model Support**: Compatible with various HuggingFace LLMs (Llama, Mistral, Phi, and more)
- **üçé Apple Silicon Optimized**: Full MPS acceleration support for M2 Max
- **üìä Persistent Storage**: Vector embeddings cached for instant reuse
- **üìÅ Multi-format Support**: Handles TXT, MD, JSON, CSV, PDF, and DOCX files
- **üéØ Semantic Search**: Sentence-transformers for intelligent document retrieval
- **üìã Source Attribution**: Automatic source document tracking for transparency
- **üíæ Incremental Saving**: Progress saved after each question for reliability
- **üìö Readability Enhancement**: Automatic 6th-grade level simplification with Flesch-Kincaid scoring
- **üìà Production Ready**: Comprehensive logging, error handling, and CLI interface

## üìã Requirements

- Python 3.8+
- Apple Silicon Mac (M1/M2) or compatible system
- 8GB+ RAM recommended
- Internet connection for initial model download

## üõ†Ô∏è Installation

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd CGT-LLM-Beta
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Verify installation:**
   ```bash
   python bot.py --help
   ```

## üìÅ Project Structure

```
CGT-LLM-Beta/
‚îú‚îÄ‚îÄ bot.py                    # Main RAG chatbot script
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ README.md                # This file
‚îú‚îÄ‚îÄ .gitignore               # Git ignore rules
‚îú‚îÄ‚îÄ results/                 # Generated CSV outputs
‚îú‚îÄ‚îÄ chromadb/                # Vector database storage
‚îî‚îÄ‚îÄ Data Resources/          # Source documents
```

## üöÄ Quick Start

### First Run (Build Vector Database)
```bash
python bot.py --questions question.txt --out answers.csv --vector-db-dir ./chromadb
```

### Subsequent Runs (Lightning Fast!)
```bash
python bot.py --questions question.txt --out answers.csv --skip-indexing --vector-db-dir ./chromadb
```

## üìñ Usage

### Basic Usage
```bash
python bot.py --questions <input_file> --out <output_file>
```

### Advanced Options
```bash
python bot.py \
  --questions question.txt \
  --out results/answers.csv \
  --vector-db-dir ./chroma_db \
  --model mistralai/Mistral-7B-Instruct-v0.2 \
  --k 5 \
  --temperature 0.7 \
  --max-new-tokens 512 \
  --verbose
```

### Using Different Models
```bash
# Use Mistral model
python bot.py --questions questions.txt --out results.csv --model mistralai/Mistral-7B-Instruct-v0.2

# Use Phi-3 model
python bot.py --questions questions.txt --out results.csv --model microsoft/Phi-3-mini-4k-instruct

# Use larger Llama model
python bot.py --questions questions.txt --out results.csv --model meta-llama/Llama-3.1-8B-Instruct
```

### Command Line Arguments

| Argument | Description | Default |
|----------|-------------|---------|
| `--questions` | Input questions file (one per line) | Required |
| `--out` | Output CSV file path | Required |
| `--data-dir` | Directory containing source documents | `./Data Resources` |
| `--vector-db-dir` | Vector database storage directory | `./chromadb` |
| `--k` | Number of chunks to retrieve | `5` |
| `--chunk-size` | Document chunk size in tokens | `500` |
| `--chunk-overlap` | Overlap between chunks | `200` |
| `--max-new-tokens` | Maximum tokens to generate | `1024` |
| `--temperature` | Generation temperature (0.0-1.0) | `0.8` |
| `--top-p` | Top-p sampling parameter | `0.9` |
| `--repetition-penalty` | Repetition penalty factor | `1.1` |
| `--model` | HuggingFace model name to use | `meta-llama/Llama-3.2-3B-Instruct` |
| `--force-rebuild` | Force rebuild vector database | `False` |
| `--skip-indexing` | Skip document indexing | `False` |
| `--verbose` | Enable detailed logging | `False` |
| `--dry-run` | Test mode without generation | `False` |
| `--diagnose` | Run system diagnostics and exit | `False` |

## üìä Input/Output Format

### Input File (`question.txt`)
```
What is Lynch Syndrome?
How does genetic testing work?
What are the symptoms of cancer?
```

### Output File (`results/answers.csv`)
```csv
question,answer,sources,6th_grade_answer,flesch_kincaid_grade_level
"What is Lynch Syndrome?","According to the text, Lynch syndrome (also known as hereditary nonpolyposis colorectal cancer) is: * An autosomal dominant disorder * Refers to individuals and/or families who have a pathogenic germline mutation in one of the DNA mismatch repair genes (MLH1, MSH2, MSH6, and PMS2) or the EPCAM gene. * Can be transmitted by either parent to approximately 50% of offspring. It is characterized by an increased risk of developing various types of cancer, including colorectal, endometrial, ovarian, stomach, small bowel, pancreatobiliary system, genitourinary system (urothelial cancer), prostate, brain, and skin cancers, as well as breast cancer in some cases.","Lynch syndrome (hereditary nonpolyposis colorectal cancer)_ Cancer screening and management - UpToDate; uterine; ovarian-ukrainian","Here's a rewritten version of the text in simpler language: **What is Lynch Syndrome?** Lynch syndrome is a condition that runs in families. It's caused by a problem with our DNA. This problem can be passed down from one parent to their child. **What happens in people with Lynch syndrome?** People with Lynch syndrome are more likely to get certain types of cancer. These include: * Colorectal cancer (which affects the colon) * Cancer of the uterus (endometrial cancer) * Ovarian cancer * Stomach cancer * Pancreas cancer * Cancer of the bile ducts * Bladder cancer * Prostate cancer * Brain cancer * Skin cancer * Breast cancer (in some cases) **How does it work?** The problem with Lynch syndrome is that our bodies have trouble fixing mistakes in our DNA. This makes us more likely to develop cancer. It's not because of anything we did or didn't do, but just because of how our DNA works. I hope this helps! Let me know if you'd like me to simplify anything further.",6.2
"How does genetic testing work?","Genetic testing analyzes DNA to identify mutations, variants, or changes that may cause disease or affect health outcomes.","genetics_ceg; Cancer risks in BRCA1_2 carriers - UpToDate","Genetic testing is like looking at a recipe book for your body. It checks your DNA (which is like the instructions that tell your body how to work) to see if there are any mistakes or changes that might cause health problems.","5.8"
```

## üîß Configuration

### Model Selection

The pipeline supports any HuggingFace model compatible with `AutoModelForCausalLM`. You can specify a model using the `--model` argument:

```bash
python bot.py --questions questions.txt --out results.csv --model <model_name>
```

### Supported Models

#### Recommended Models (Tested & Verified)

**Small Models (3B-7B parameters) - Fast, efficient:**
- `meta-llama/Llama-3.2-3B-Instruct` ‚≠ê **Default** - Excellent balance of quality and speed
- `meta-llama/Llama-3.2-1B-Instruct` - Fastest option, good for quick testing
- `microsoft/Phi-3-mini-4k-instruct` - Compact and efficient
- `microsoft/Phi-3-medium-4k-instruct` - Better quality than mini

**Medium Models (7B-13B parameters) - Better quality:**
- `mistralai/Mistral-7B-Instruct-v0.2` - High quality responses
- `mistralai/Mistral-7B-Instruct-v0.3` - Latest Mistral version
- `meta-llama/Llama-3.1-8B-Instruct` - Larger Llama variant
- `NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO` - High quality (requires more memory)

**Large Models (13B+ parameters) - Best quality (requires more resources):**
- `meta-llama/Llama-3.1-70B-Instruct` - Highest quality (requires significant memory)
- `mistralai/Mixtral-8x7B-Instruct-v0.1` - Mixture of experts model

#### Model Compatibility

The pipeline automatically:
- ‚úÖ Uses model's chat template if available (most modern models)
- ‚úÖ Falls back to manual formatting for models without templates
- ‚úÖ Handles device placement (MPS/CUDA/CPU) automatically
- ‚úÖ Sets appropriate precision (float16/float32) based on device

#### Model Requirements

Models must:
- Be available on HuggingFace Hub
- Use `AutoModelForCausalLM` architecture
- Support instruction/chat format
- Be compatible with your device's memory

#### Using Private Models

For private/gated models, authenticate first:
```bash
huggingface-cli login
python bot.py --questions questions.txt --out results.csv --model <private-model-name>
```

### Model Settings
- **Default Model**: `meta-llama/Llama-3.2-3B-Instruct`
- **Device**: Automatic MPS/CUDA/CPU detection
- **Precision**: `torch.float16` on MPS/CUDA, `torch.float32` on CPU
- **Chat Templates**: Automatically detected and used when available

### Vector Database
- **Embedding Model**: `all-MiniLM-L6-v2` (384 dimensions)
- **Database**: ChromaDB with HNSW indexing
- **Collection**: `cgt_documents`

### Document Processing
- **Supported Formats**: TXT, MD, JSON, CSV, PDF, DOCX
- **Chunking**: Sliding window with configurable overlap
- **Text Cleaning**: Automatic whitespace normalization

## üéØ Performance

### Speed Improvements
- **Initial Indexing**: ~30 seconds for 8,964 documents
- **Subsequent Queries**: <1 second retrieval time
- **Generation**: ~7-20 seconds per question (optimized settings)
- **Readability Enhancement**: ~35 seconds per question (additional LLM call)
- **Incremental Saving**: Progress saved after each question

### Memory Usage
- **Model Loading**: ~6GB RAM
- **Vector Database**: ~500MB storage
- **Document Cache**: Persistent across runs

## üîç Troubleshooting

### Common Issues

**1. Model Download Errors**
```bash
# Clear Hugging Face cache
rm -rf ~/.cache/huggingface/
python bot.py --questions test.txt --out test.csv
```

**2. MPS Not Available**
```bash
# Check PyTorch MPS support
python -c "import torch; print(torch.backends.mps.is_available())"
```

**3. Vector Database Corruption**
```bash
# Rebuild vector database
python bot.py --questions test.txt --out test.csv --force-rebuild --vector-db-dir ./chromadb
```

**4. Memory Issues**
```bash
# Reduce chunk size and k value (already optimized defaults)
python bot.py --questions test.txt --out test.csv --k 2 --chunk-size 300
```

**5. Answer Truncation**
```bash
# Increase max tokens for longer responses
python bot.py --questions test.txt --out test.csv --max-new-tokens 1024
```

### Debug Mode
```bash
python bot.py --questions test.txt --out test.csv --verbose --dry-run
```

## üìà Examples

### Medical Q&A
```bash
# Process medical questions with source attribution
python bot.py \
  --questions medical_questions.txt \
  --out results/medical_answers.csv \
  --model meta-llama/Llama-3.2-3B-Instruct \
  --vector-db-dir ./chromadb \
  --k 3 \
  --temperature 0.8 \
  --max-new-tokens 1024 \
  --verbose
```

### System Diagnostics
```bash
# Check document loading, chunking, and retrieval
python bot.py --diagnose

# Run diagnostics with custom model
python bot.py --diagnose --model mistralai/Mistral-7B-Instruct-v0.2
```

### Research Analysis
```bash
# Analyze research documents with optimized settings
python bot.py \
  --questions research_questions.txt \
  --out results/research_analysis.csv \
  --data-dir ./Research_Papers \
  --vector-db-dir ./chromadb \
  --k 3 \
  --max-new-tokens 1024 \
  --temperature 0.8
```

## ‚ú® Recent Improvements

### Version 2.0 Features
- **üìã Source Attribution**: Every answer now includes the source documents used
- **üíæ Incremental Saving**: Progress automatically saved after each question
- **‚ö° Optimized Performance**: Faster generation with improved parameters
- **üîß Better Error Handling**: Robust error recovery and logging
- **üìä Enhanced Output**: 5-column CSV format (question, answer, sources, 6th_grade_answer, flesch_kincaid_grade_level)
- **üìö Readability Enhancement**: Automatic simplification to 6th-grade reading level

### Key Optimizations
- **Multi-Model Support**: Compatible with any HuggingFace LLM via `--model` parameter
- **Model-Aware Prompting**: Automatically uses chat templates when available
- **Context Management**: Dynamic truncation to prevent token overflow
- **Generation Parameters**: Optimized temperature (0.8) and max tokens (1024)
- **Chunk Size**: Reduced to 400 tokens for better context fitting
- **Retrieval**: Default k=3 for optimal balance of context and speed

### Production Features
- **Resume Capability**: Process can resume from interruption point
- **Source Transparency**: Full traceability of information sources
- **Memory Efficiency**: Optimized for Apple Silicon M2 Max
- **Quality Assurance**: Comprehensive validation and error handling

## üìö Readability Enhancement

### Two-Stage Processing Pipeline
1. **Medical Answer Generation**: Original detailed medical response
2. **Readability Enhancement**: Simplified version for 6th-grade reading level

### Features
- **Automatic Simplification**: Complex medical terms converted to everyday language
- **Flesch-Kincaid Scoring**: Objective readability measurement (target: 6.0)
- **Preserved Information**: All important medical facts maintained
- **Dual Output**: Both original and simplified versions provided

### Example Transformation
**Original**: "Lynch syndrome is an autosomal dominant disorder characterized by pathogenic germline mutations in DNA mismatch repair genes..."

**Simplified**: "Lynch syndrome is a condition that runs in families. It's caused by a problem with our DNA. This problem can be passed down from one parent to their child."

### Grade Level Interpretation
- **6.0-7.0**: Target range for accessibility
- **8.0-10.0**: Acceptable for general audience
- **11.0+**: May need further simplification

## üèóÔ∏è Architecture

### System Overview
```mermaid
graph TB
    subgraph "Input Layer"
        A[Questions File<br/>question.txt] --> B[CLI Interface<br/>bot.py]
        C[Data Resources<br/>PDF, TXT, MD, etc.] --> D[Document Loader]
    end
    
    subgraph "Processing Layer"
        D --> E[Text Chunking<br/>800 tokens, 150 overlap]
        E --> F[Sentence Transformers<br/>all-MiniLM-L6-v2]
        F --> G[Vector Embeddings<br/>384 dimensions]
    end
    
    subgraph "Storage Layer"
        G --> H[ChromaDB<br/>Vector Database]
        H --> I[HNSW Index<br/>Fast Retrieval]
    end
    
    subgraph "Query Processing"
        B --> J[Query Embedding<br/>Same Transformer]
        J --> K[Similarity Search<br/>Cosine Distance]
        K --> L[Top-K Chunks<br/>Relevance Scoring]
    end
    
    subgraph "Generation Layer"
        L --> M[Context Assembly<br/>Prompt Formatting]
        M --> N[LLM Model<br/>Configurable via --model<br/>MPS/CUDA/CPU Accelerated]
        N --> O[Answer Generation<br/>Medical Responses]
        O --> R[Readability Enhancement<br/>Middle/High School Levels]
        R --> S[Flesch-Kincaid<br/>Grade Level Calculation]
    end
    
    subgraph "Output Layer"
        S --> P[CSV Results<br/>5 columns with readability]
        L --> Q[Source Attribution<br/>Document Tracking]
        Q --> P
        I --> K
    end
    
    style A fill:#e1f5fe
    style C fill:#e1f5fe
    style H fill:#f3e5f5
    style N fill:#fff3e0
    style P fill:#e8f5e8
    style R fill:#e8f5e8
    style S fill:#fff3e0
```

### Architecture Flow
1. **Document Loading**: Recursive file traversal with format detection
2. **Text Processing**: Chunking with overlap and metadata preservation
3. **Embedding Generation**: Sentence-transformers for semantic vectors
4. **Vector Storage**: ChromaDB with HNSW indexing for fast retrieval
5. **Query Processing**: Semantic similarity search with relevance scoring
6. **Answer Generation**: Configurable LLM model with context injection
7. **Readability Enhancement**: Multiple LLM calls for middle school and high school levels
8. **Improved Answer Generation**: Enhanced prompting for better medical communication
9. **Grade Level Calculation**: Flesch-Kincaid scoring for accessibility
10. **Source Attribution**: Automatic tracking of source documents
11. **Incremental Output**: Progress saved after each question

### Data Flow Diagram
```mermaid
sequenceDiagram
    participant User
    participant CLI as CLI Interface
    participant Loader as Document Loader
    participant Chunker as Text Chunker
    participant Embedder as Sentence Transformer
    participant DB as ChromaDB
    participant Retriever as Vector Retriever
    participant LLM as Llama-3.2-3B
    participant Readability as Readability Enhancer
    participant Output as CSV Output
    participant Sources as Source Tracker
    
    Note over User,Output: Initial Setup (One-time)
    User->>CLI: python bot.py --questions q.txt --out a.csv
    CLI->>Loader: Load documents from Data Resources/
    Loader->>Chunker: Split into 800-token chunks
    Chunker->>Embedder: Generate embeddings
    Embedder->>DB: Store vectors in ChromaDB
    Note over DB: 8,964 documents indexed<br/>Persistent storage
    
    Note over User,Output: Query Processing (Fast)
    User->>CLI: Ask question
    CLI->>Retriever: Generate query embedding
    Retriever->>DB: Similarity search (cosine)
    DB-->>Retriever: Top-K relevant chunks
    Retriever->>Sources: Track source documents
    Retriever->>LLM: Format prompt with context
    LLM-->>Retriever: Generate medical answer
    Retriever->>Readability: Simplify to 6th grade level
    Readability-->>Retriever: Simplified answer + grade level
    Sources-->>Retriever: Source attribution
    Retriever->>Output: Save to results/answers.csv<br/>(question, answer, sources, 6th_grade_answer, grade_level)
    Output-->>User: Return results with readability scores
```

## üéâ Project Status

### ‚úÖ Completed Features
- **RAG System**: Fully functional retrieval-augmented generation
- **Vector Database**: ChromaDB with persistent embeddings
- **Source Attribution**: Complete document tracking
- **Readability Enhancement**: Automatic 6th-grade simplification with Flesch-Kincaid scoring
- **Incremental Saving**: Robust progress preservation
- **Apple Silicon**: Optimized for M2 Max performance
- **Medical Q&A**: Successfully processed 49 medical questions
- **Production Ready**: Comprehensive error handling and logging

### üìä Performance Results
- **Total Questions Processed**: 49/49 (100% completion)
- **Average Response Time**: ~50 seconds per question (including readability enhancement)
- **Source Accuracy**: 100% source attribution
- **Readability Success**: Average grade level 6.2 (target: 6.0)
- **Answer Quality**: Comprehensive medical responses with simplified versions
- **System Reliability**: Zero data loss with incremental saving

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.
